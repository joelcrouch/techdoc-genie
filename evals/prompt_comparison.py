import json
import sys
from pathlib import Path

# Add the project root to the Python path
sys.path.insert(0, str(Path(__file__).parent.parent))

from src.retrieval.vector_store import VectorStore
from src.agent.rag_chain import RAGChain
from src.ingestion.embedder import DocumentEmbedder
from src.utils.config import get_settings
from src.utils.logger import setup_logger

logger = setup_logger(__name__)

def run_prompt_comparison():
    """
    Compares responses generated by the RAG chain using different prompt templates.
    """
    logger.info("Starting prompt comparison script...")

    # --- Setup: Load Vector Store ---
    try:
        settings = get_settings()
        embedder = DocumentEmbedder(
            provider=settings.embedding_provider,
            model=settings.embedding_model
        )

        # Assuming the default PostgreSQL vector store
        # This name is derived from the build script's default settings
        POSTGRES_VECTOR_STORE_NAME = f"vectorstore_chunk{settings.chunk_size}_overlap{settings.chunk_overlap}"
        vector_store_dir = f"data/vector_store/{POSTGRES_VECTOR_STORE_NAME}"
        
        vector_store = VectorStore(embedder=embedder.embedder, persist_path=vector_store_dir)
        vector_store.load(name=POSTGRES_VECTOR_STORE_NAME)
        
        logger.info(f"Vector store '{POSTGRES_VECTOR_STORE_NAME}' loaded successfully from {vector_store_dir}.")
    except Exception as e:
        logger.error(f"Failed to load vector store: {e}", exc_info=True)
        logger.error("Please ensure the default PostgreSQL vector store is built.")
        sys.exit(1)

    # --- Test Queries ---
    test_queries = [
        "How do I create an index in PostgreSQL?",
        "What's the difference between INNER JOIN and LEFT JOIN?",
        "My query is slow, how can I optimize it?"
    ]

    # --- Prompt Types to Compare ---
    prompt_types = ["base", "detailed", "troubleshooting", "code"]

    for query in test_queries:
        print("\n" + "="*80)
        print(f"QUERY: {query}")
        print("="*80 + "\n")
        
        for prompt_type in prompt_types:
            print(f"\n--- {prompt_type.upper()} TEMPLATE ---")
            logger.info(f"Querying with prompt type: {prompt_type} for query: {query}")
            
            try:
                # Initialize RAG chain with local Ollama provider and current prompt type
                rag_chain = RAGChain(
                    vector_store=vector_store, 
                    llm_provider_type="ollama",
                    model_id="phi3:mini",
                    prompt_type=prompt_type,
                    retrieval_k=3
                )
                
                result = rag_chain.query(query) # Using .query() to get raw result without citation formatting
                print(result['result'])
                print("\n" + "-"*80 + "\n") # Separator for readability
            except Exception as e:
                logger.error(f"Error processing query '{query}' with prompt '{prompt_type}': {e}", exc_info=True)
                print(f"ERROR: Could not generate response for this prompt type due to: {e}")
                print("\n" + "-"*80 + "\n")

    logger.info("Prompt comparison script finished.")

if __name__ == "__main__":
    run_prompt_comparison()
