# Sprint 1: Local LLM Integration and Evaluation Enhancements

This document summarizes the significant accomplishments, architectural changes, debugging efforts, and evaluation framework development during this session, building upon the initial Sprint 1 plan.

---

## 1. Local LLM Integration

**Objective:** Integrate a local, resource-efficient Large Language Model (LLM) into the RAG pipeline to run on constrained GPU hardware.

*   **Model Selection:** Identified and integrated **`phi3:mini`** (approx. 3.8B parameters) as a suitable local LLM due to its efficiency and compatibility with 4GB VRAM when quantized, running via **Ollama**.
*   **Provider Development:**
    *   Created `src/agent/providers/ollama_provider.py` to handle communication with the local Ollama server. This provider implements a `BaseLLMProvider` interface.
    *   Modified `src/utils/config.py` to include `ollama_timeout` and `ollama_base_url` settings for Ollama API calls.
*   **LangChain Compatibility Layer:**
    *   Developed `src/agent/llm_wrapper.py`, a `CustomLLM` class that acts as an adapter, making our `BaseLLMProvider` (like `OllamaProvider`) compatible with LangChain's expected `LLM` interface.
*   **Flexible RAG Chain:**
    *   Modified `src/agent/rag_chain.py` to dynamically instantiate either an `OpenAI` (`ChatOpenAI`) or `Ollama` (wrapped `CustomLLM`) provider based on runtime arguments.

---

## 2. RAG Pipeline Assembly & Debugging

**Objective:** Implement the core RAG pipeline components as per `sprint1.md` and ensure their functionality with the new local LLM integration.

*   **Core Component Creation:**
    *   Created `src/agent/prompts.py` to define various prompt templates for different use cases.
    *   Created `src/agent/citation_manager.py` to standardize citation tracking and response formatting.
    *   Created `src/agent/rag_chain.py` as the central orchestrator of the RAG process (retrieval, prompt construction, LLM call, response formatting).
*   **Interactive Query Interface:**
    *   Developed `src/agent/query_interface.py`, an interactive command-line tool for querying the RAG system. This tool supports dynamic selection of LLM provider (`--provider`), LLM model (`--model`), prompt type (`--prompt`), and vector store (`--vector-store`).
*   **Extensive Debugging & Refinement:**
    *   **`ImportError` (HuggingFaceEmbedder):** Corrected a mismatch in class naming (`HuggingFaceEmbedder` vs `HuggingFaceEmbeddingProvider`) in `src/ingestion/embedder.py`.
    *   **`RepositoryNotFoundError` (`.env` override):** Identified and resolved a configuration override issue where an incorrect embedding model was specified in the `.env` file, causing HuggingFace model download failures.
    *   **`FileNotFoundError` (Vector Store Pathing):** Fixed `src/agent/query_interface.py` to dynamically construct the correct, nested vector store path as generated by the build scripts, resolving issues with loading non-default vector stores.
    *   **Successful RAG Execution:** Verified successful end-to-end RAG functionality with both the default PostgreSQL documentation and a newly ingested Ubuntu PDF document.

---

## 3. Vector Store Ingestion Enhancements

**Objective:** Improve the flexibility and usability of the vector store ingestion process.

*   **Monolithic PDF Handling:** Diagnosed and corrected issues with the CLI `build-vectorstore` command when dealing with single monolithic PDF files that lacked a `.pdf` extension.  Details: The initial download script downloaded the ubuntu docs in a file just named pdf not ubuntu_docs.pdf or something like that.  So I renamed it.  Not sure if i addresssed the core issue.  Ill try with another doc src and see how that goes.
*   **Generic Build Script:** Created `scripts/build_generic_vectorstore.py` to provide a more direct and flexible method for ingesting documents and building vector stores, bypassing some of the Click CLI complexities for specific use cases.

---

## 4. Evaluation Framework Development

**Objective:** Build tools for systematically evaluating both the LLM's prompt effectiveness and the underlying retrieval mechanism.

*   **End-to-End Integration Test:**
    *   Developed `tests/integration/test_rag_pipeline_e2e.py` using `pytest` to validate the entire RAG pipeline from vector store loading to LLM response generation with assertions on content and citations.
*   **Prompt Comparison Script:**
    *   Created `evals/prompt_comparison.py` to compare how different prompt templates influence LLM answers for a given set of queries.
    *   Addressed `ReadTimeout` issues during LLM generation by increasing `ollama_timeout` in `config.py` from 60 to 180 seconds.
*   **Retrieval Evaluation Script:**
    *   Developed `evals/retrieval_evaluation.py` to objectively measure the effectiveness of various chunking strategies (size, overlap, method) in retrieving relevant documents.
    *   This script calculates **Hit Rate** and **Mean Reciprocal Rank (MRR)**, allowing for deterministic tuning of the retrieval component, decoupled from LLM variability.
    *   Conducted initial experiments showing larger recursive chunk sizes (up to 4096 tokens) improved retrieval performance.
    *   Discussed the critical relationship between `RETRIEVAL_K` and the interpretation of Hit Rate/MRR scores, and its impact on the actual RAG chain.  RE: MRR captiures the variability if we cahnge k values (number of docs returned), and determines how 'good' the retireval is.  Im not convinced the scores are representative of what is actually occurring. the correct resuslt is being retunned,but is comparign its result against the 'ground truth'-perhaps a semantic llm as a judge is appropirate here. VADER perhaps.

---

## 5. Documentation

*   **Quickstart Guide:** Updated `docs/quickstart_guide.md` to include comprehensive instructions for:
    *   Starting/managing the Ollama server.
    *   Building new vector stores (using the new generic script).
    *   Querying the RAG assistant with various LLM providers, models, prompts, and specific vector stores.
    *   Running both the `prompt_comparison.py` and `retrieval_evaluation.py` evaluation scripts.

---

## Key Learnings & Insights

*   **Modularity is Key:** The architecture developed (BaseLLMProvider, CustomLLM wrapper) proved crucial for easily swapping LLM backends (OpenAI vs. Ollama).
*   **Debugging Local LLMs:** Timeouts and resource constraints (VRAM) are common challenges when running LLMs locally, requiring careful configuration.
*   **Layered Evaluation:** Separating retrieval evaluation (deterministic, numerical) from full RAG evaluation (qualitative, LLM-dependent) is essential for effective tuning.
*   **Chunking Matters:** Initial retrieval evaluation demonstrated that chunking strategy significantly impacts retrieval accuracy, with larger recursive chunks showing better performance in early tests.
*   **Iterative Process:** Developing and debugging the RAG pipeline is an iterative process, requiring continuous testing and refinement.